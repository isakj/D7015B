{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3, D7015B, Industrial AI and eMaintenance - Part I: Theories & Concepts #\n",
    "\n",
    "Isak Jonsson, isak.jonsson@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import re\n",
    "import statistics\n",
    "import json\n",
    "from wordcloud import WordCloud\n",
    "import string\n",
    "from itertools import filterfalse\n",
    "import nlu\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment-specific settings, for getting Spark to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Users\\\\isakj\\\\.jdks\\\\corretto-11.0.22\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\projects\\\\winutils\\\\hadoop-3.2.0\"\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\\\projects\\\\spark-3.2.3-bin-hadoop3.2\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"HADOOP_HOME\"] + \"/bin:\" + os.environ[\"SPARK_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\"\n",
    "os.environ['SPARK_LOCAL_IP'] = '127.0.0.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open file ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('openDamagesinTrains.xls')\n",
    "\n",
    "print(df.shape)\n",
    "print(dict(df).keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reparsing date ##\n",
    "\n",
    "The code below is not used, but it plots the day-of-month values for all reporting dates cells where the cell actually is a date cell. Statistically, the days should be evenly spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = ax.hist(df[df['Damage reporting date']\n",
    "        .apply(lambda x : isinstance(x, datetime.datetime))]['Damage reporting date']\n",
    "        .apply(lambda x : x.day), \n",
    "        bins=np.linspace(1,31,31))\n",
    "\n",
    "ax.set_xlabel('Day of month')\n",
    "ax.set_ylabel('Occurrences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reparsing\n",
    "* If cell is a datetime, create a new datetime object with day and months swapped.\n",
    "* If cell is text and can be parsed as M/D/YYYY, create a new datetime value with those elements.\n",
    "* If cell value is `'Öppen'`, store as `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(value):\n",
    "    if isinstance(value, datetime.datetime):\n",
    "        return datetime.datetime(value.year, value.day, value.month)\n",
    "    if value=='Öppen':\n",
    "        return None\n",
    "    m=re.fullmatch('(\\\\d{1,2})/(\\\\d{1,2})/(\\\\d{4})', value)\n",
    "    return datetime.datetime(int(m.group(3)),int(m.group(1)),int(m.group(2)))\n",
    "\n",
    "df['Damage reporting date'] = df['Damage reporting date'].apply(parse_date)\n",
    "df['Damage closing date'] = df['Damage closing date'].apply(parse_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTF and TTR extraction ##\n",
    "\n",
    "Two arrays (DataFrames) are created. The axes are the vehicle code and the damage category, respectively. For each axis, a special value `'*'` is used to hold values for all vehicles and categories. The value in the TTF array is for each combination a list of the difference to last reported failure. The value in the TTR array is for each combination the difference between the reporting and closing date for those rows in the dataset where a closing date appears.\n",
    "\n",
    "Note on storage. This, rather lazy, way of storing the TTF and TTR values are $O(\\#failures)$, where $\\#failures$ is the number of rows in the original dataset. If only mean and variance is required, a more memory-efficient way would be to store $n,\\sum{x},\\sum{x^2}$ for each combination, bring memory requirements down to $O(\\#vehicles \\times \\#categories)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.sort_values(\"Damage reporting date\", inplace=True) # to make diff work\n",
    "\n",
    "vehicles = ['*'] + list(df['Vehicle'].sort_values().unique())\n",
    "categories = ['*'] + list(df['Damage category'].sort_values().unique())\n",
    "\n",
    "\n",
    "ttf = pd.DataFrame('', vehicles, categories)\n",
    "ttr = pd.DataFrame('', vehicles, categories)\n",
    "\n",
    "for vehicle in vehicles:\n",
    "    for category in categories:\n",
    "        # find the applicable rows\n",
    "        rows = df.apply(lambda x : \n",
    "                        (vehicle == '*' or x['Vehicle'] == vehicle) and\n",
    "                        (category == '*' or x['Damage category'] == category), axis=1)\n",
    "        ttf.loc[vehicle,category] = list(df[rows]['Damage reporting date'].diff()[1:].map(lambda x:x.days))\n",
    "        closed = df[rows & df.notnull()['Damage closing date']]\n",
    "        ttr.loc[vehicle,category] = list((closed['Damage closing date']-closed['Damage reporting date']).map(lambda x:x.days))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle = 'F26005'\n",
    "category = 'ALLMÄN FORDONSINFORMATION'\n",
    "vehicles = [ vehicle ]\n",
    "categories = [ category ]\n",
    "\n",
    "rows = df.apply(lambda x : \n",
    "                        (vehicle == '*' or x['Vehicle'] == vehicle) and\n",
    "                        (category == '*' or x['Damage category'] == category), axis=1)\n",
    "df[rows]\n",
    "list(df[rows]['Damage reporting date'].diff()[1:].map(lambda x:x.days))\n",
    "print(list(df[rows]['Damage reporting date'].diff()[1:].map(lambda x:x.days)), sep=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the values below can be changed\n",
    "vehicle = 'F26005'\n",
    "category = 'ALLMÄN FORDONSINFORMATION'\n",
    "#category = 'BROMSSYSTEM'\n",
    "logarithmic = False\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "values = ttf.loc[vehicle,category]\n",
    "ax.hist(values, bins=20)\n",
    "if logarithmic:\n",
    "    plt.yscale('log')\n",
    "ax.set_xlabel('Days ($\\\\bar{{x}}={0:.2f}$, $\\\\tilde{{x}}={1:.1f}$, $\\\\sigma={2:.2f}$, $N={3}$)'.format(statistics.mean(values), statistics.median(values), statistics.stdev(values), len(values)))\n",
    "ax.set_ylabel('Occurrences')\n",
    "ax.set_title('Time-to-failure; ' + ('all vehicles' if vehicle=='*' else vehicle) + ', ' + ('all damage categories' if category=='*' else category))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "values = ttr.loc[vehicle,category]\n",
    "ax.hist(values, bins=20)\n",
    "if logarithmic:\n",
    "    plt.yscale('log')\n",
    "ax.set_xlabel('Days ($\\\\bar{{x}}={0:.2f}$, $\\\\tilde{{x}}={1:.1f}$, $\\\\sigma={2:.2f}$, $N={3}$)'.format(statistics.mean(values), statistics.median(values), statistics.stdev(values), len(values)))\n",
    "ax.set_ylabel('Occurrences')\n",
    "ax.set_title('Time-to-repair; ' + ('all vehicles' if vehicle=='*' else vehicle) + ', ' + ('all damage categories' if category=='*' else category))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data in tabular form ###\n",
    "\n",
    "With every thing as lists in dataframe, it is very easy to do statistical analyses for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttf.applymap(len)\n",
    "ttf.applymap(statistics.mean)\n",
    "ttf.applymap(statistics.stdev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word cloud ##\n",
    "\n",
    "The data category field is rather \"noisy\", when it comes to create a word cloud. \n",
    "In order to clean it up a bit, we want to remove everything that is _too_ specific,\n",
    "like the number of millimeter a door gap is.\n",
    "Also, we want to remove words that are not specific at all, so called _stop words_.\n",
    "\n",
    "One of the challenges is that the text is in Swedish, and as expected, the amount of NLP libraries\n",
    "for Swedish is much lower than English NLP libraries. The strategy I ended up with is:\n",
    "\n",
    "1. split everything into words, just by splitting on space\n",
    "2. make everything lowercase\n",
    "3. remove every \"word\" that has a digit it it\n",
    "4. remove every word that starts with a `/` (it is a signature)\n",
    "5. remove leading and trailing punctuation, which can lead to empty words, which are deleted\n",
    "6. remove all stopwords in https://github.com/stopwords-iso/stopwords-sv\n",
    "7. convert all words to a string and lemmatize the string using https://sparknlp.org/2020/05/05/lemma_sv.html\n",
    "8. remove stopwords, again, using https://sparknlp.org/2020/07/14/stopwords_sv.html\n",
    "\n",
    "One would think that the correct order might be to first lemmatize, then remove stopwords.\n",
    "But one specific problem I found was the word \"för\" in the the adverb meaning \"too\": \"gapet är för stort\".\n",
    "However, the lemmatizer treated it as the verb \"för\" (\"bring\"), replacing it with the infinitive form\n",
    "\"föra\", which was not caught by the stopword algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_sv = json.load(open('stopwords-sv/stopwords-sv.json'))\n",
    "rows = df.apply(lambda x : isinstance(x['Damage description'], str), axis=1)\n",
    "words = df[rows]['Damage description'].apply(str.split).sum()\n",
    "# everything lowercase\n",
    "words = [x.lower() for x in words]\n",
    "# remove all numbers\n",
    "words = list(filter(lambda x : not re.match('.*[0-9].*', x), words))\n",
    "# remove signatures '/...'\n",
    "words = list(filter(lambda x : not re.match('^/.*', x), words))\n",
    "# remove remaining leading and trailing punctuation\n",
    "words = [x.strip(string.punctuation) for x in words]\n",
    "# remove empty words\n",
    "words = list(filter(None, words))\n",
    "# remove stopwords from JSON list\n",
    "words = list(filterfalse(stopwords_sv.__contains__, words))\n",
    "\n",
    "text = ' '.join(words)\n",
    "\n",
    "nlu_stopwords = nlu.load(\"sv.stopwords\")\n",
    "nlu_lemma = nlu.load('sv.lemma')\n",
    "\n",
    "# lemmatize words\n",
    "words = nlu_lemma.predict([text], output_level='document').lem[0]\n",
    "# remove stopwords, again\n",
    "words = list(nlu_stopwords.predict(' '.join(words))['stopword_less'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(\n",
    "    stopwords={}, \n",
    "    normalize_plurals=False, \n",
    "    random_state=1,\n",
    "    width=1200,\n",
    "    height=600).generate(' '.join(words))\n",
    "oldsize = plt.rcParams[\"figure.figsize\"]\n",
    "plt.rcParams[\"figure.figsize\"] = (wordcloud.width/100,wordcloud.height/100)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.rcParams[\"figure.figsize\"] = oldsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning for category prediction ##\n",
    "\n",
    "We use a [TD-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) classifier for a simple way of matching words with labels (categories).\n",
    "\n",
    "Before vectorizing the data, we clean it by using pretty much the same operation as for the word cloud. When it comes to lemmatizing, I have tested both the WordNet lemmatizer and the NLU lemmatizer.\n",
    "\n",
    "* The WordNet lemmatizer is for English, so it should not perform very well.\n",
    "* The NLU lemmatizer is for Swedish. However, it is very slow per call, so I build a larger string with delimiters (`'/#/'`) for values, run the lemmatizer, and then split the result.\n",
    "\n",
    "Not suprisingly, the Swedish lemmatizer is better than the English one. But the difference is marginal:\n",
    "\n",
    "* No lemmatizer: 52% accuracy\n",
    "* WordNet lemmatizer: 54% accuracy\n",
    "* NLU lemmatizer: 55% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_nlu_lemmatizer = True\n",
    "use_wordnet_lemmatizer = False\n",
    "\n",
    "import nltk\n",
    "import sklearn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Tokenization och preprocessing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('swedish'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    words = text.split()\n",
    "    words = list(filter(lambda x : not re.match('.*[0-9].*', x), words))\n",
    "    # remove signatures '/...'\n",
    "    words = list(filter(lambda x : not re.match('^/.*', x), words))\n",
    "    # remove remaining leading and trailing punctuation\n",
    "    words = [x.strip(string.punctuation) for x in words]\n",
    "    # remove empty words\n",
    "    words = list(filter(None, words))\n",
    "    # remove stopwords from JSON list\n",
    "    words = list(filterfalse(stop_words.__contains__, words))\n",
    "    words = list(filterfalse(stopwords_sv.__contains__, words))\n",
    "    text = ' '.join(words)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    if use_wordnet_lemmatizer:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df_clean = df[df['Damage description'].apply(lambda x : isinstance(x, str))]\n",
    "\n",
    "if use_nlu_lemmatizer:\n",
    "    sss = ' /#/ '.join(df_clean['Damage description'].apply(preprocess_text))\n",
    "    lem = nlu_lemma.predict(sss, output_level='document')\n",
    "    X = ' '.join(lem['lem'][0]).split('/#/')\n",
    "else:\n",
    "    X = list(df_clean['Damage description'].apply(preprocess_text))\n",
    "\n",
    "y = list(df_clean['Damage category'])\n",
    "# TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "# Training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# predict the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# find the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions results ###\n",
    "\n",
    "First some interpretations from the classifier, what are the most \"important\" words for each damage category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names_out()\n",
    "DD = pd.DataFrame()\n",
    "for i, doc in enumerate(classifier.classes_):\n",
    "    D = pd.DataFrame({'word':vectorizer.get_feature_names_out(), 'score':classifier.feature_count_[i,:]})\n",
    "    D.sort_values('score', inplace=True, ascending=False)\n",
    "    DD[doc] = list(D['word'])\n",
    "DD[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, some test results for made up sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    sentence = preprocess_text(sentence)\n",
    "    if use_nlu_lemmatizer:\n",
    "        lem = nlu_lemma.predict(sentence, output_level='document')\n",
    "        sentence = ' '.join(lem['lem'][0])\n",
    "    new_sentence_vectorized = vectorizer.transform([sentence])\n",
    "    predicted_label = classifier.predict(new_sentence_vectorized)[0]\n",
    "    return predicted_label\n",
    "\n",
    "new_sentence = \"Dörr 41 har väldigt högt pip. Måste sänkas en aning, skär i öronen.\"\n",
    "print(\"Predicted Label for '{}': {}\".format(new_sentence, predict(new_sentence)))\n",
    "\n",
    "new_sentence = \"Handfat har spruckit mitt itu!\"\n",
    "print(\"Predicted Label for '{}': {}\".format(new_sentence, predict(new_sentence)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
